package org.tensorframes.impl

import java.util.concurrent.atomic.AtomicInteger

import scala.collection.JavaConverters._
import org.apache.spark.SparkContext
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.sql.expressions.UserDefinedFunction
import org.apache.spark.sql.{Column, Row, TFUDF}
import org.apache.spark.sql.functions.struct
import org.apache.spark.sql.types.StructType
import org.tensorflow.Session
import org.tensorflow.framework.GraphDef
import org.tensorflow.Graph
import org.tensorframes.{ColumnInformation, Shape, ShapeDescription, _}


/**
 * Column transforms. These are not as efficient as working with full dataframes,
 * but they may be simpler to work with.
 */
// TODO: this is mostly cut and paste from performMapRows -> should combine
object SqlOps extends Logging {
  import SchemaTransforms.{get, check}

  // Counter: the number of sessions currently opened.
  private class LocalState(
      val session: Session,
      val graphHash: Int,
      val graph: Graph,
      val counter: AtomicInteger) {
    def close(): Unit = {
      session.close()
      graph.close()
    }
  }

  // A map of graph hash -> state for this graph.
  private[this] var current: Map[Int, LocalState] = Map.empty
  private[this] val lock = new Object()

  // The maximum number of sessions that can be opened concurrently.
  val maxSessions: Int = 5

  /**
   * Experimental: expresses a Row transform as a SQL-registrable UDF.
   *
   * This is not as efficient as doing a direct dataframe transform, and it leaks
   * some resources after the completion of the transform. These resources may easily be
   * be reclaimed though.
   */
  def makeUDF(
      graph: GraphDef,
      shapeHints: ShapeDescription): UserDefinedFunction = {
    val summary = TensorFlowOps.analyzeGraphTF(graph, shapeHints)
      .map(x => x.name -> x).toMap
    val inputs = summary.filter(_._2.isInput)
    val outputs = summary.filter(_._2.isOutput)

    // The output schema of the block from the data generated by TF.
    val outputTFSchema: StructType = {
      // The order of the output columns is decided for now by their names.
      val fields = outputs.values.toSeq.sortBy(_.name).map { out =>
        // The shapes we get in each output node are the shape of the cells of each column, not the
        // shape of the column. Add Unknown since we do not know the exact length of the block.
        val blockShape = out.shape.prepend(Shape.Unknown)
        ColumnInformation.structField(out.name, out.scalarType, blockShape)
      }
      StructType(fields.toArray)
    }

    val outputSchema: StructType = StructType(outputTFSchema)

    def processColumn(inputColumn: Column): Any => Row = {
      // Special case: if the column has a single non-structural field and if
      // there is a single input in the graph, we automatically wrap the input in a structure.
      (inputs.keySet.toSeq, inputColumn.expr.dataType) match {
        case (_, _: StructType) => processColumn0(inputColumn)
        case (Seq(name1), _) => processColumn0(struct(inputColumn.alias(name1)))
        case (names, dt) =>
          throw new Exception(s"Too many graph inputs for the given column type: names=$names, dt=$dt")
      }
    }

    def processColumn0(inputColumn: Column): Any => Row = {
      val inputSchema = inputColumn.expr.dataType match {
        case st: StructType => st
        case x: Any => throw new Exception(
          s"Only structures are currently accepted: given $x")
      }
      val fieldsByName = inputSchema.fields.map(f => f.name -> f).toMap
      val cols = inputSchema.fieldNames.mkString(", ")

      // Initial check of the input.
      inputs.values.foreach { in =>
        val fname = get(shapeHints.inputs.get(in.name),
          s"The graph placeholder ${in.name} was not given a corresponding dataframe field name as input:" +
            s"hints: ${shapeHints.inputs}")

        val f = get(fieldsByName.get(fname),
          s"Graph input ${in.name} found, but no column to match it. Dataframe columns: $cols")

        val stf = get(ColumnInformation(f).stf,
          s"Data column ${f.name} has not been analyzed yet, cannot run TF on this dataframe")

        check(stf.dataType == in.scalarType,
          s"The type of node '${in.name}' (${stf.dataType}) is not compatible with the data type " +
            s"of the column (${in.scalarType})")

        val cellShape = stf.shape.tail
        // No check for unknowns: we allow unknowns in the first dimension of the cell shape.
        check(cellShape.checkMorePreciseThan(in.shape),
          s"The data column '${f.name}' has shape ${stf.shape} (not compatible) with shape" +
            s" ${in.shape} requested by the TF graph")

        check(in.isPlaceholder,
          s"Invalid type for input node ${in.name}. It has to be a placeholder")
      }

      // The column indices requested by TF, and the name of the placeholder that gets fed.
      val requestedTFInput: Array[(NodePath, Int)] = {
        val colIdxs = inputSchema.fieldNames.zipWithIndex.toMap
        inputs.keys.map { nodePath =>
          val fieldName = shapeHints.inputs(nodePath)
          nodePath -> colIdxs(fieldName)
        }   .toArray
      }

      logger.debug(s"makeUDF: input schema = $inputSchema, requested cols: ${requestedTFInput.toSeq}" +
        s" complete output schema = $outputSchema")
      // TODO: this is leaking the file.
      val sc = SparkContext.getOrCreate()
      val gProto = sc.broadcast(TensorFlowOps.graphSerial(graph))
      val f = performUDF(inputSchema, requestedTFInput, gProto, outputTFSchema)
      f
    }

    TFUDF.make1(processColumn, outputSchema)
  }

  def performUDF(
    inputSchema: StructType,
    inputTFCols: Array[(NodePath, Int)],
    g_bc: Broadcast[SerializedGraph],
    tfOutputSchema: StructType): Any => Row = {

    logger.debug(s"performUDF: inputSchema=$inputSchema inputTFCols=${inputTFCols.toSeq}")

    def f(in: Any): Row = {
      val row = in match {
        case r: Row => r
        case x => Row(x)
      }
      val g = g_bc.value
      retrieveSession(g) { session =>
        g.evictContent()
        val inputTensors = TFDataOps.convert(row, inputSchema, inputTFCols)
        logger.debug(s"performUDF:inputTensors=$inputTensors")
        val requested = tfOutputSchema.map(_.name)
        var runner = session.runner()
        for (req <- requested) {
          runner = runner.fetch(req)
        }
        for ((inputName, inputTensor) <- inputTensors) {
          runner = runner.feed(inputName, inputTensor)
        }
        val outs = runner.run().asScala
        logger.debug(s"performUDF:outs=$outs")
        // Close the inputs
        inputTensors.map(_._2).foreach(_.close())
        val res = TFDataOps.convertBack(outs, tfOutputSchema, Array(row), inputSchema, appendInput = false)
        // Close the outputs
        outs.foreach(_.close())
        assert(res.hasNext)
        val r = res.next()
        assert(!res.hasNext)
        //      logDebug(s"performUDF: r=$r")
        r
      }
    }
    f
  }

  private def retrieveSession[T](g: SerializedGraph)(f: Session => T): T = {
    //  // This is a better version that uses the hash of the content, but it requires changes to
    //  // TensorFrames. Only use with version 0.2.9+
    val hash = java.util.Arrays.hashCode(g.content)
    retrieveSession(g, hash, f)
  }

  private def retrieveSession[T](g: SerializedGraph, gHash: Int, f: Session => T): T = {
    // Do some cleanup first:
    lock.synchronized {
      val numberOfSessionsToClose = Math.max(current.size - maxSessions, 0)
      // This is best effort only, there may be more sessions opened at some point.
      if (numberOfSessionsToClose > 0) {
        // Find some sessions to close: they are not currently used, and they are not the requested session.
        val sessionsToRemove = current.valuesIterator
          .filter { s => s.counter.get() == 0 && s.graphHash != gHash }
          .take(numberOfSessionsToClose)
        for (state <- sessionsToRemove) {
          logger.debug(s"Removing session ${state.graphHash}")
          state.close()
          current = current - state.graphHash
        }
      }
    }

    // Now, try to retrieve the session, or create a new one.
    // TODO: use a double lock mechanism or a lazy value, since importing a graph may take a long time.
    val state = lock.synchronized {
      val state0 = current.get(gHash) match {
        case None =>
          // Add a new session
          val tg = new Graph()
          tg.importGraphDef(g.content)
          val s = new Session(tg)
          val ls = new LocalState(s, gHash, tg, new AtomicInteger(0))
          current = current + (gHash -> ls)
          ls
        case Some(ls) =>
          // Serve the existing session
          ls
      }
      // Increment the counter in the locked section, to guarantee that the session does not get collected.
      state0.counter.incrementAndGet()
      state0
    }

    // Perform the action
    try {
      f(state.session)
    } finally {
      state.counter.decrementAndGet()
    }
  }
}
