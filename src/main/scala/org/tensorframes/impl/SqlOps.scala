package org.tensorframes.impl

import scala.collection.JavaConverters._
import org.apache.spark.SparkContext
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.sql.expressions.UserDefinedFunction
import org.apache.spark.sql.{Column, Row, TFUDF}
import org.apache.spark.sql.types.StructType
import org.tensorflow.Session
import org.tensorflow.framework.GraphDef
import org.tensorflow.Graph
import org.tensorframes.{ColumnInformation, Shape, ShapeDescription, _}


/**
 * Column transforms. These are not as efficient as working with full dataframes,
 * but they may be simpler to work with.
 */
// TODO: this is mostly cut and paste from performMapRows -> should combine
object SqlOps extends Logging {
  import SchemaTransforms.{get, check}

  private class LocalState(val session: Session, val graphHash: Int, val graph: Graph)

  private[this] var current: Option[LocalState] = None

  /**
   * Experimental: expresses a Row transform as a SQL-registrable UDF.
   *
   * This is not as efficient as doing a direct dataframe transform, and it leaks
   * some resources after the completion of the transform. These resources may easily be
   * be reclaimed though.
   */
  def makeUDF(
      graph: GraphDef,
      shapeHints: ShapeDescription): UserDefinedFunction = {
    val summary = TensorFlowOps.analyzeGraphTF(graph, shapeHints)
      .map(x => x.name -> x).toMap
    val inputs = summary.filter(_._2.isInput)
    val outputs = summary.filter(_._2.isOutput)

    // The output schema of the block from the data generated by TF.
    val outputTFSchema: StructType = {
      // The order of the output columns is decided for now by their names.
      val fields = outputs.values.toSeq.sortBy(_.name).map { out =>
        // The shapes we get in each output node are the shape of the cells of each column, not the
        // shape of the column. Add Unknown since we do not know the exact length of the block.
        val blockShape = out.shape.prepend(Shape.Unknown)
        ColumnInformation.structField(out.name, out.scalarType, blockShape)
      }
      StructType(fields.toArray)
    }

    val outputSchema: StructType = StructType(outputTFSchema)


    def processColumn(inputColumn: Column): Row => Row = {
      val inputSchema = inputColumn.expr.dataType match {
        case st: StructType => st
        case x: Any => throw new Exception(
          s"Only structures are currently accepted: given $x")
      }
      val fieldsByName = inputSchema.fields.map(f => f.name -> f).toMap
      val cols = inputSchema.fieldNames.mkString(", ")

      // Initial check of the input.
      inputs.values.foreach { in =>
        val fname = get(shapeHints.inputs.get(in.name),
          s"The graph placeholder ${in.name} was not given a corresponding dataframe field name as input:" +
            s"hints: ${shapeHints.inputs}")

        val f = get(fieldsByName.get(fname),
          s"Graph input ${in.name} found, but no column to match it. Dataframe columns: $cols")

        val stf = get(ColumnInformation(f).stf,
          s"Data column ${f.name} has not been analyzed yet, cannot run TF on this dataframe")

        check(stf.dataType == in.scalarType,
          s"The type of node '${in.name}' (${stf.dataType}) is not compatible with the data type " +
            s"of the column (${in.scalarType})")

        val cellShape = stf.shape.tail
        // No check for unknowns: we allow unknowns in the first dimension of the cell shape.
        check(cellShape.checkMorePreciseThan(in.shape),
          s"The data column '${f.name}' has shape ${stf.shape} (not compatible) with shape" +
            s" ${in.shape} requested by the TF graph")

        check(in.isPlaceholder,
          s"Invalid type for input node ${in.name}. It has to be a placeholder")
      }

      // The column indices requested by TF, and the name of the placeholder that gets fed.
      val requestedTFInput: Array[(NodePath, Int)] = {
        val colIdxs = inputSchema.fieldNames.zipWithIndex.toMap
        inputs.keys.map { nodePath =>
          val fieldName = shapeHints.inputs(nodePath)
          nodePath -> colIdxs(fieldName)
        }   .toArray
      }

      logTrace(s"mapRows: input schema = $inputSchema, requested cols: ${requestedTFInput.toSeq}" +
        s" complete output schema = $outputSchema")
      // TODO: this is leaking the file.
      val sc = SparkContext.getOrCreate()
      val gProto = sc.broadcast(TensorFlowOps.graphSerial(graph))
      val f = performUDF(inputSchema, requestedTFInput, gProto, outputTFSchema)
      f
    }

    new TFUDF(processColumn, outputSchema)
  }

  def performUDF(
    inputSchema: StructType,
    inputTFCols: Array[(NodePath, Int)],
    g_bc: Broadcast[SerializedGraph],
    tfOutputSchema: StructType): Row => Row = {

    def f(row: Row): Row = {
      val g = g_bc.value
      val session = retrieveSession(g)
      g.evictContent()

      val inputTensors = TFDataOps.convert(row, inputSchema, inputTFCols)
      logDebug(s"performUDF:inputTensors=$inputTensors")
      val requested = tfOutputSchema.map(_.name)
      var runner = session.runner()
      for (req <- requested) {
        runner = runner.fetch(req)
      }
      for ((inputName, inputTensor) <- inputTensors) {
        runner = runner.feed(inputName, inputTensor)
      }
      val outs = runner.run().asScala
      logDebug(s"performUDF:outs=$outs")
      // Close the inputs
      inputTensors.map(_._2).foreach(_.close())
      val res = TFDataOps.convertBack(outs, tfOutputSchema, Array(row), inputSchema, appendInput = true)
      // Close the outputs
      outs.foreach(_.close())
      assert(res.hasNext)
      val r = res.next()
      assert(!res.hasNext)
      r
    }
    f
  }

  private def retrieveSession(g: SerializedGraph): Session = current match {
    case None =>
      val tg = new Graph()
      tg.importGraphDef(g.content)
      val s = new Session(tg)
      current = Some(new LocalState(s, g.dataHashCode, tg))
      s
    case Some(ls) if ls.graphHash == g.dataHashCode =>
      // Reuse the current session
      ls.session
    case Some(ls) =>
      // Close the current session and open a new one.
      ls.session.close()
      ls.graph.close()
      current = None
      retrieveSession(g)
  }


}
